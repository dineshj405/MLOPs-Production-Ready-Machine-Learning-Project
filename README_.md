# US Visa Approval Prediction --- MLOps Production-Ready ML Project

A fully scalable, modular, and production-ready Machine Learning
pipeline designed using **MLOps best practices**.\
This project covers everything from data ingestion to deployment,
following industry standards for reproducibility, automation, and
maintainability.

------------------------------------------------------------------------

## ğŸš€ Features

-   Modular project architecture\
-   Config-driven pipeline\
-   Automated data ingestion & validation\
-   Data transformation pipeline\
-   Model training with hyperparameter tuning\
-   Custom components & pipelines\
-   Logging, exception handling & utilities\
-   CI/CD ready structure\
-   Easy installation & setup using Conda

------------------------------------------------------------------------

## ğŸ“ Project Structure

    us_visa/
    â”‚
    â”œâ”€â”€ components/
    â”‚   â”œâ”€â”€ data_ingestion.py
    â”‚   â”œâ”€â”€ data_validation.py
    â”‚   â”œâ”€â”€ data_transformation.py
    â”‚   â”œâ”€â”€ model_trainer.py
    â”‚   â””â”€â”€ __init__.py
    â”‚
    â”œâ”€â”€ entity/
    â”œâ”€â”€ constants/
    â”œâ”€â”€ pipeline/
    â”œâ”€â”€ main.py
    â””â”€â”€ __init__.py

    artifacts/  (Auto-generated after pipeline runs)

------------------------------------------------------------------------

## ğŸ› ï¸ Tech Stack

-   **Python 3.8**
-   **Conda** for environment management
-   **Scikit-learn**, **pandas**, **numpy**
-   **MLflow** (optional integration)
-   **MLOps modular architecture**

------------------------------------------------------------------------

## ğŸ”§ Installation & Setup

### 1ï¸âƒ£ Create Conda Environment

``` bash
conda create -n visa python=3.8 -y
```

### 2ï¸âƒ£ Activate Environment

``` bash
conda activate visa
```

### 3ï¸âƒ£ Install Dependencies

``` bash
pip install -r requirements.txt
```

------------------------------------------------------------------------

## â–¶ï¸ How to Run the Project

### Run the entire ML pipeline:

``` bash
python main.py
```

This executes:

1.  **Data Ingestion**\
2.  **Data Validation**\
3.  **Data Transformation**\
4.  **Model Training**

Outputs are stored in the **artifacts/** folder.

------------------------------------------------------------------------

## ğŸ“Œ Workflow Description

### **1. Constants**

Contains static values like paths, filenames, and column names.

### **2. Entity**

Defines input/output schemas for different pipeline stages.

### **3. Components**

Core building blocks of the ML pipeline:\
- `data_ingestion.py`\
- `data_validation.py`\
- `data_transformation.py`\
- `model_trainer.py`

### **4. Pipeline**

Scripts that connect components in an end-to-end flow.

### **5. Main File**

Entry point that triggers the complete process.

------------------------------------------------------------------------

## ğŸ§ª Testing

You can add `pytest` or unit test modules later for validating
components and pipelines.

------------------------------------------------------------------------

## ğŸ“ Git Commands

``` bash
git add .
git commit -m "Updated"
git push origin main
```

------------------------------------------------------------------------

## ğŸ™Œ Author

**Dinesh Jamdade**\
MLOps Engineer \| Data Scientist
